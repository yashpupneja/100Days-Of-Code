{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Vector and matrices play a central role in data science: they are probably the most common way of representing data to be analyzed and manipulated by virtually any machine learning or analytics algorithm.  However, it is also important to understand that there really two uses to matrices within data science:\n",
    "\n",
    "1. Matrices are the \"obvious\" way to store tabular data (particularly when all the data is numeric) in an efficient manner.\n",
    "2. Matrices are the foundation of linear algebra, which the \"language\" of most machine learning and analytics algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices for tabular data and row/column ordering\n",
    "\n",
    "Let's start with a simple example representing tabular data using matrices, one of the more natural ways to represent such data (and as we will see in later lectures, one of the ways that lends itself well to use in machine learning).  Let's consider the \"Grades\" table that we previously discussed in our presentation of relational data:\n",
    "\n",
    "| Person ID | HW1 Grade | HW2 Grade |\n",
    "| :---: | :---: | :---: |\n",
    "| 5 | 85 | 95 | \n",
    "| 6 | 80 | 60 |\n",
    "| 100 | 100 | 100 |\n",
    "\n",
    "Ignoring the primary key column (this is not really a numeric feature, so makes less sense to store as a real-valued number), we could represent the data in the table via the matrix \n",
    "\\begin{equation}\n",
    "A \\in \\mathbb{R}^{3 \\times 2} = \\left[ \\begin{array}{cc} 85 & 95 \\\\ 80 & 60 \\\\ 100 & 100 \\end{array} \\right ]\n",
    "\\end{equation}\n",
    "\n",
    "While there seems to be little else that can be said at this point, there is actually a subtle but important point about how the data in this table is really laid out in memory.  Since data in memory is laid out sequentially (at least logically as far as programs are concerned, if not physically on the chip) we can opt to store the data in _row major order_, that is, storing each row sequentially\n",
    "\\begin{equation}\n",
    "(85, 95, 80, 60, 100, 100)\n",
    "\\end{equation}\n",
    "or in _column major order_, storing each column sequentially\n",
    "\\begin{equation}\n",
    "(85, 80, 100, 95, 60, 100)\n",
    "\\end{equation}\n",
    "\n",
    "Row major ordering is the default for 2D arrays in C (and the default for the Numpy library), while column major ordering is the default for FORTRAN.  There is no obvious reason to prefer one over the order, but due to the cache locality in CPU memory hierarchies, the different methods will be able to access the data more efficiently by row or by column respectively.  Most importantly, however, the real issue is that because a large amount of numerical code was originally (and still is) written in FORTRAN, if you ever want to call external numerical code, there is a good chance you'll need to worry about the ordering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of linear algebra\n",
    "\n",
    "In addition to serving as a method for storing tabular data, vector and matrices also provide a method for studying sets of linear equations.  These notes here are going to provide a very brief overview and summary of some of the primary linear algebra notation that you'll need for this course.  But it is really meant to be a refresher for those who already have some experience with linear algebra previously.  If you do not, then I have previously put out an online mini-course covering (with notes) some of the basics of linear algebra: [Linear Algebra Review](http://www.cs.cmu.edu/~zkolter/course/linalg/).  This course honestly goes a bit beyond what is needed for this particular course, but it can act a good reference.\n",
    "\n",
    "Consider the following two linear equations in two variables $x_1$ and $x_2$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "4 x_1 - 5 x_2 & = -13 \\\\\n",
    "-2 x_1 + 3 x_2 & = 9\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This can written compactly as the equation $A x = b$, where\n",
    "\\begin{equation}\n",
    "A \\in \\mathbb{R}^{2 \\times 2} = \\left [ \\begin{array}{cc} 4 & -5 \\\\ -2 & 3 \\end{array} \\right ], \n",
    "\\;\\; b \\in \\mathbb{R}^2 = \\left [ \\begin{array}{c} -13 \\\\ 9 \\end{array} \\right ], \n",
    "\\;\\; x \\in \\mathbb{R}^2 = \\left [ \\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right ].\n",
    "\\end{equation}\n",
    "As this hopefully illustrates, one of the entire points of linear algebra is to make the notation and math _simpler_ rather than more complex.  However, until you get used to the conventions, writing large equations where the size of matrices/vectors are always implicit can be a bit tricky, so you'll some care is needed to make sure you do not include any incorrect derivations.\n",
    "\n",
    "\n",
    "### Basic operations and special matrices\n",
    "\n",
    "**Addition and substraction**: Matrix addition and subtraction are applied elementwise over the matrices, and can only apply two two matrices of the same size.  That is, if $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{m \\times n}$ then their sum/difference $C = A + B$ is another matrix of the same size $C \\in \\mathbb{R}^{m \\times n}$ where\n",
    "\\begin{equation}\n",
    "C_{ij} = A_{ij} + B_{ij}.\n",
    "\\end{equation}\n",
    "\n",
    "**Transpose**: Transposing a matrix flips its rows and columns.  That is, if $A \\in \\mathbb{R}^n$, then it's transpose, denoted $C = A^T$ is a matrix $C \\in \\mathbb{R}^{n \\times m}$ where\n",
    "\\begin{equation}\n",
    "C_{ij} = A_{ji}.\n",
    "\\end{equation}\n",
    "\n",
    "**Matrix multiplication**: Matrix multiplication is a bit more involved.  Unlike addition and subtraction, matrix multiplication does not perform elementwise multiplication of the two matrices.  Instead, for a matrix $A \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{n \\times p}$ (note these precise sizes, as they are important), their product $C = AB$ is a matrix $C \\in \\mathbb{R}^{m \\times p}$ where\n",
    "\\begin{equation}\n",
    "C_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}.\n",
    "\\end{equation}\n",
    "In order for this sum to make sense, notice that the number of columns in $A$ must equal the number of rows in $B$.  If you'd like a bit more of the intuition behind why matrix multiplication is defined this way, the notes above provide some important interpretations.  It's important to note the following properties, though:\n",
    "\n",
    "* Matrix multiplication is associative: $(AB)C = A(BC)$ (i.e., it doesn't matter in what order you do the multiplications, though it _can_ matter from a computational perspective, as some orderings will be more efficient to compute than others)\n",
    "* Matrix multiplication is distributive: $A(B+C) = AB + AC$\n",
    "* Matrix multiplication is _not_ commutative: $AB \\neq BA$. This is really true in two different ways.  Under the above matrix sizes, the multiplication $BA$ is not a valid expression if $m \\neq p$ (since the number of columns in $B$ would not match the number of rows in $A$).  And even if the dimensions _do_ match (for instance if all the matrices were $n \\times n$) the products will still not be equal in general.\n",
    "\n",
    "**Identity matrix**: The identity matrix $I \\in \\mathbb{R}^{n \\times n}$ is a square matrix with ones on the diagonal an zeros everywhere else\n",
    "\\begin{equation}\n",
    "I = \\left [ \\begin{array}{cccc} \n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "0 & 0 & \\cdots & 1\n",
    "\\end{array} \\right ].\n",
    "\\end{equation}\n",
    "\n",
    "It has the property that for any matrix $A \\in \\mathbb{R}^{m \\times n}$\n",
    "\\begin{equation}\n",
    "A I = I A = A\n",
    "\\end{equation}\n",
    "where we note that the two $I$ matrices in the above equations are actually two _different_ sizes (the first one is $ n \\times n$ and the second is $m \\times m$, to make the math work right).  For this reason, some people use the notation $I_n$ to explicitly denote the size of $I$, but it is not really needed, because the size that any identity must be can be inferred from the other matrices in the equation.\n",
    "\n",
    "**Matix inverse**: For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, the matrix inverse $A^{-1} \\in \\mathbb{R}^{n \\times n}$ is the unique matrix such that\n",
    "\\begin{equation}\n",
    "A^{-1}A = A A^{-1} = I.\n",
    "\\end{equation}\n",
    "The matrix inverse need not exist for all square matrices (it will depend on the linear independence between rows/columns of $A$, and we will consider such possibilities a bit later in the course.\n",
    "\n",
    "**Solving linear equations**: The matrix inverse provides an immediate method to obtain the solution to systems of linear equations.  Recall out example above of a set of linear equations $A x = b$.  If we want to find the $x$ that satisfies this equation, we multiply both sizes of the equation by $A^{-1}$ on the left to get\n",
    "\\begin{equation}\n",
    "A^{-1}A x = A^{-1}b \\Longrightarrow x = A^{-1} b.\n",
    "\\end{equation}\n",
    "The nice thing here is that as far as we are concerned in this course, the set of equations is now _solved_.  We don't have to worry about any actual operations that you may have learned about to actually obtain this solution (scaling the linear equations by some constant, adding/subtracting them to construct a solution, etc).  The linear algebra libraries we will use do all this for us, and our only concern is getting the solution into a form like the one above.\n",
    "\n",
    "**Transpose of matrix product**: It follows immediately from the definition of matrix multiplication and the transpose that\n",
    "\\begin{equation}\n",
    "(AB)^T = B^T A^T\n",
    "\\end{equation}\n",
    "i.e., the transpose of a matrix product is the product of the transposes, in reverse order.\n",
    "\n",
    "**Inverse of matrix**: It also follows immediately from the definitions that for $A,B\\in \\mathbb{R}^{n \\times n}$ both square\n",
    "\\begin{equation}\n",
    "(AB)^{-1} = B^{-1} A^{-1}\n",
    "\\end{equation}\n",
    "i.e. the inverse of a matrix product is the product of the inverses, in reverse order.\n",
    "\n",
    "**Inner products**: One type of matrix multiplication is common enough that it deserves special mention.  If $x,y \\in \\mathbb{R}^n$ are vectors of the same dimensions, then\n",
    "\\begin{equation}\n",
    "x^T y = \\sum_{i=1}^n x_i y_i\n",
    "\\end{equation}\n",
    "(the matrix product of $x$ transposed, i.e., a row vector and $y$, a column vector) is a _scalar_ quantity called the inner product of $x$ and $y$; it is simply equal to the sum of the corresponding elements of $x$ and $y$ multiplied together.\n",
    "\n",
    "**Vector norms**: These are only slightly related to vectors matrices, but this seems like a good place to introduce it.  We will use the notation \n",
    "\\begin{equation}\n",
    "\\|x\\|_2 = \\sqrt{x^T x} = \\sqrt{\\sum_{i=1}^n x_i^2}\n",
    "\\end{equation}\n",
    "to denote the Euclidean (also called $\\ell_2$) norm of $x$.  We may occasionally also refer to the $\\ell_1$ norm \n",
    "\\begin{equation}\n",
    "\\|x\\|_1 = \\sum_{i=1}^n |x_i|\n",
    "\\end{equation}\n",
    "or the $\\ell_\\infty$ norm\n",
    "\\begin{equation}\n",
    "\\|x\\|_\\infty = \\max_{i=1,\\ldots,n} |x_i|.\n",
    "\\end{equation}\n",
    "\n",
    "** Complexity of operations**:  For making efficient use of matrix operations, it is extremely important to know the big-O complexity of the different matrix operations.  Immediately from the definitions of the operations, assuming $A,B \\in \\mathbb{R}^{n \\times n}$ and $x,y \\in \\mathbb{R}^n$ we have the the following complexities:\n",
    "\n",
    "* Inner product $x^Ty$: $O(n)$\n",
    "* Matrix-vector product $Ax$: $O(n^2)$\n",
    "* Matrix-matrix product $AB$: $O(n^3)$\n",
    "* Matrix inverse $A^{-1}$, or matrix solve $A^{-1}y$ (as we will emphasize below, these are arctually done differently; they both have the same big-O omplexity, but different concstant terms on the runtime in practice): $O(n^3)$\n",
    "\n",
    "Note that the big-O complexity along with the associative property of matrix multiplications implies very different complexities for computing the exact same term in different ways.  For example, suppose we want to compute the matrix products $ABx$.  We could compute this as $(AB)x$ (computing the $AB$ product first, then multiplying with $x$); this approach would have complexity $O(n^3)$, as the matrix-matrix product would dominate the computation.  On the other hand, if we compute the product as $A(Bx)$ (first computing the vector product $Bx$, which produces a vector, then multiplying this by $A$), the complex is only $O(n^2)$, as we just have two matrix-vector products.  As you can imagine, these orders of operations therefore make a huge difference in terms of the time complexity of linear algebra operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "blis_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "blas_opt_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_lapack_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_opt_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__config__.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output may not be exactly the same as what is shown here, but you should be able to infer from this if you're using an optimized library like (int this case), Intel MKL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating numpy arrays\n",
    "\n",
    "The `ndarray` is the basic data type in Numpy.  These can be created the `numpy.array` command, passing a 1D list of number to create a vector or a 2D list of numbers to create an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-13   9] \n",
      "\n",
      "[[ 4 -5]\n",
      " [-2  3]]\n"
     ]
    }
   ],
   "source": [
    "b = np.array([-13,9])            # 1D array construction\n",
    "A = np.array([[4,-5], [-2,3]])   # 2D array contruction\n",
    "print(b, \"\\n\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.] \n",
      "\n",
      "[0. 0. 0. 0.] \n",
      "\n",
      "[ 0.37966604 -0.20033789  0.60336991 -0.10875717]\n"
     ]
    }
   ],
   "source": [
    "print(np.ones(4), \"\\n\")           # 1D array of ones\n",
    "print(np.zeros(4), \"\\n\")          # 1D array of zeros\n",
    "print(np.random.randn(4))         # 1D array of random normal numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]] \n",
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] \n",
      "\n",
      "[[-1.97032344 -1.93175964 -0.80062094 -1.41193599]\n",
      " [-0.98072469 -0.46961101 -0.5910408  -0.17823062]\n",
      " [-1.88902428 -0.76459014  0.65972551 -0.52488768]]\n"
     ]
    }
   ],
   "source": [
    "print(np.ones((3,4)), \"\\n\")       # 2D array of ones\n",
    "print(np.zeros((3,4)), \"\\n\")      # 2D array of zeros\n",
    "print(np.random.randn(3,4))       # 2D array of random normal numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]] \n",
      "\n",
      "[[ 0.71847871  0.          0.        ]\n",
      " [ 0.          1.05793919  0.        ]\n",
      " [ 0.          0.         -0.15207004]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(3),\"\\n\")                     # create array for 3x3 identity matrix\n",
    "print(np.diag(np.random.randn(3)),\"\\n\")   # create diagonal array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing into numpy arrays\n",
    "\n",
    "You can index into Numpy arrays in many different ways.  The most common is to index into them as you would a list: using single indices and using slices, with the additional consideration that using the `:` character will select the entire span along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "5 \n",
      "\n",
      "[4 5 6] \n",
      "\n",
      "[[4 5 6]\n",
      " [7 8 9]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "print(A, \"\\n\")\n",
    "print(A[1,1],\"\\n\")           # select singe entry\n",
    "print(A[1,:],\"\\n\")           # select entire row\n",
    "print(A[1:3, :], \"\\n\")       # slice indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]]\n"
     ]
    }
   ],
   "source": [
    "print(A[1:2,1:2])  # Select A[1,1] as a singleton 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy also support fancier indexing with _integer_ and _Boolean_ indexing.  If we create another array or list of indices (that  is, for the rows in above array, this would be integers between 0-3 (inclusive)), then we can use this list of integers to select the rows/columns we want to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(A[[1,2,3],:])  # select rows 1, 2, and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these integer indices do not need to be in order, nor do they have to include at most once instance of each row/column; we can use this notation to repeat rows/columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 8 9]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "print(A[[2,1,2],:])  # select rows 2, 1, and 2 again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also use an array of integers instead of a list, for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 8 9]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "print(A[np.array([2,1,2]),:])  # select rows 2, 1, and 2 again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Last, we can also use Boolean indexing.  If we specify a list or array of booleans that is the _same size_ as the corresponding row/column, then the Boolean values specify a \"mask\" over which values are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5  6]\n",
      " [10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(A[[False, True, False, True],:])  # Select 1st and 3rd rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note, be careful if you try to use integer or boolean indexing for both dimensions.  This will attempt to select generate a 1D array of entries with both those locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(A[[2,1,2],[1,2,0]])    # the same as np.array([A[2,1], A[1,2], A[2,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you actually want to first select based upon rows, then upon columns, you'll do it like the following, essentially doing each indexing separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 9, 7],\n",
       "       [5, 6, 4],\n",
       "       [8, 9, 7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[[2,1,2],:][:,[1,2,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations on arrays\n",
    "\n",
    "Arrays can be added/subtracted, multiplied/divided, and transposed, but these are _not_ all the same as their linear algebra counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4]\n",
      " [ 5  7  7]\n",
      " [10  9 12]\n",
      " [11 15 13]] \n",
      "\n",
      "[[ 0  1  2]\n",
      " [ 3  3  5]\n",
      " [ 4  7  6]\n",
      " [ 9  7 11]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[1, 1, 1], [1,2,1], [3, 1, 3], [1, 4, 1]])\n",
    "print(A+B, \"\\n\") # add A and B elementwise (same as \"standard\" matrix addition)\n",
    "print(A-B) # subtract B from A elementwise (same as \"standard\" matrix subtraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array multiplication and division are done _elementwise_, they are _not_ matrix multiplication or anything related to matrix inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4 10  6]\n",
      " [21  8 27]\n",
      " [10 44 12]] \n",
      "\n",
      "[[ 1.          2.          3.        ]\n",
      " [ 4.          2.5         6.        ]\n",
      " [ 2.33333333  8.          3.        ]\n",
      " [10.          2.75       12.        ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(A*B, \"\\n\") # elementwise multiplication, _not_ matrix multiplication\n",
    "print(A/B, \"\\n\") # elementwise division, _not_ matrix inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can transpose arrays, but note this _only_ has meaning for 2D (or higher) arrays.  Transposing a 1D array doesn't do anything, since Numpy has no notion of column vectors vs. row vectors for 1D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  7 10]\n",
      " [ 2  5  8 11]\n",
      " [ 3  6  9 12]] \n",
      "\n",
      "[1 2 3 4] \n",
      "\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "print(A.T, \"\\n\")\n",
    "print(x, \"\\n\")\n",
    "print(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.ones((4,3))          # A is 4x3\n",
    "x = np.array([[1,2,3]])      # x is 1x3\n",
    "A*x                          # repeat x along dimension 4 (repeat four times), and add to A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [2., 2., 2.],\n",
       "       [3., 3., 3.],\n",
       "       [4., 4., 4.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1],[2],[3],[4]])\n",
    "A*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `x` has size (4,1), so it is effectively resized to (4,3) along the second dimension, repeating values along the columns.  This has the effect of scaling the rows of `A` by `x`.\n",
    "\n",
    "As a final note, the rule for numpy is that if the two arrays being operated upon have _different_ numbers of dimensions, we extend the dimensions in the _leading_ dimensions to all implicitly be 1.  Thus, the following code will implicitly consider `x` (which is a 1D array of size 3), to be a (1,3) array, and then apply the broadcasting rules, which thus has the same effect as our first broadcasting example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "A*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to implicitly \"cast\" a n sized 1D array to a (n,1) sized array, we can use the notation `x[:,None]` (we put \"None\" for the dimensions we want to define to be 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]] \n",
      "\n",
      "[[1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "print(x[:,None], \"\\n\")\n",
    "print(A*x[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules can be confusing, and it takes some time to get used to them, but the advantage of broadcasting is that you can compute many operations quite efficiently, and once you get used to the notation, it is actually not the difficult to understand what is happening.  For example, from a \"linear algebra\" perspective, the right way to scale the column of a matrix is a matrix multiplication by a diagonal matrix, like in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.diag(np.array([1,2,3]))\n",
    "A @ D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra operations\n",
    "\n",
    "Starting with Python 3, there is now a matrix multiplication operator `@` defined between numpy arrays (previously one had to use the more cumbersome `np.dot()` function to accomplish the same thing).  Note that in the following example, all the array sizes are created such that the matrix multiplications work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41337324 -0.66350059 -1.82147782]\n",
      " [-2.16998825 -2.40241711 -4.33623392]\n",
      " [-0.2981898  -1.57940933  1.1248706 ]\n",
      " [-0.71716303 -0.51486958 -1.42210433]\n",
      " [-2.04281995  0.75511802 -1.79024823]] \n",
      "\n",
      "[ 2.97639907  5.99140973 -1.72678964  2.24818291  3.40525744] \n",
      "\n",
      "2.6485992174939375\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(5,4)\n",
    "C = np.random.randn(4,3)\n",
    "x = np.random.randn(4)\n",
    "y = np.random.randn(5)\n",
    "z = np.random.randn(4)\n",
    "\n",
    "print(A @ C, \"\\n\")       # matrix-matrix multiply (returns 2D array)\n",
    "print(A @ x, \"\\n\")       # matrix-vector multiply (returns 1D array)\n",
    "print(x @ z)       # inner product (scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important point to note, though, here. Depending on the sizes of the arrays passed to the `@` operator, numpy will return results of different sizes: two 2D arrays result in a 2D array (matrix-matrix product), a 2D array and a 1D array result in a 1D array (matrix-vector product), and two 1D arrays result in a scalar (just a floating point number, not an `ndarray` at all).  This can cause some issues if, for instance, your code always assumes that the result of a `@` operation between two `ndarray` objects will also return an `ndarray`: depending on the size of the arrays you pass (i.e., if they are both 1D arrays), you will actually get a floating point object, not an `ndarray` at all.\n",
    "\n",
    "The rules can be especially confusing when we think about multiplying vectors on the left of matrices, i.e., forming a matrix-vector product $y^T A$ for $y \\in \\mathbb{R}^m$, $A \\in \\mathbb{R}^{m \\times n}$.  This is a valid matrix product, but since Numpy has no distinction between column and row vectors, both the following operations compute the same 1D result (i.e., which performs the above left-multplication, but return the result just as a 1D array): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.97607489 -2.25833661 -1.40402785  2.36255964] \n",
      "\n",
      "[-0.97607489 -2.25833661 -1.40402785  2.36255964]\n"
     ]
    }
   ],
   "source": [
    "print(A.T @ y, \"\\n\")\n",
    "print(y.T @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusing part is that because transposes have no meaning to for 1D arrays, the following code _also_ returns the same result, despite $y A$ not being a valid linear algebra expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.97607489 -2.25833661 -1.40402785  2.36255964]\n"
     ]
    }
   ],
   "source": [
    "print(y @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, trying to do the multiplication in the other order $Ay$ (which is also not a valid linear algebra expression), does throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,4) and (5,) not aligned: 4 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-305a31fe747d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,4) and (5,) not aligned: 4 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "print(A @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are oddities that you will get used to, and while I initially thought that the notation for everything here was rather counter-intuitive, it actually does make sense (in some respect) why everything was implemented this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "A = sp.coo_matrix(np.eye(5))\n",
    "A.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can cast it to an `ndarray` using the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(A.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of matrix multiplication operators\n",
    "\n",
    "Let's also look at what we mentioned above, considering the order of matrix multiplications in terms of time complexity.  By default the `@` operator will be applied left-to-right, which may result is very inefficient orders for the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.5 ms ± 7.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(1000,1000)\n",
    "B = np.random.randn(1000,2000)\n",
    "x = np.random.randn(2000)\n",
    "%timeit A @ B @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs the matrix products $(AB)x$, which computes the inefficient matrix multiplication first.  If we want to compute the product in the much more efficient order $A(Bx)$, we would use the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18 ms ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit A @ (B @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The later operation can be about 50x faster that the first version, and the difference only gets larger for larger matrices.  Be _very_ careful about this point when you are multiplying big matrices and vectors together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverses and linear solves\n",
    "\n",
    "Finally, Numpy includes the routine `np.linalg.inv()` for computing the matrix inverse $A^{-1}$ and `np.linalg.solve()` for computing the matrix solve $A^{-1}b$, for $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5 2.5]\n",
      " [1.  2. ]] \n",
      "\n",
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "b = np.array([-13,9])\n",
    "A = np.array([[4,-5], [-2,3]])\n",
    "\n",
    "print(np.linalg.inv(A), \"\\n\")   # explicitly form inverse\n",
    "print(np.linalg.solve(A,b))     # compute solution A^{-1}b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the `np.linalg.solve()` routine is also equivalent to the matrix-vector product with the inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.inv(A) @ b)    # don't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [2, 4, 1, 3, 1, 1]\n",
    "row_indices = [1, 3, 2, 0, 3, 1]\n",
    "column_indices = [0, 0, 1, 2, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compressed sparse column (CSC) format.** The downsides of the above approach motivate a different matrix storage, where we _do_ explicitly maintain a column-major ordering of the non-zero entries.  However, if we are going to do so, then it turns out we actually get a more efficient structure if we change the nature of the `column_indices` array.  Instead of an $\\mathrm{nnz}$-dimensional array containing the column index of each entry, we make the array be a $(n+1)$-dimenional array (remember that $n$ is the number of columns in the matrix), pointing to the _index_ of the starting location for each column in the `row_indices` and `values` array.\n",
    "\n",
    "This is a bit hard to understand at first, so an example can make things more understandable.  Instead of the COO format above, the CSC format consists of the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [2, 4, 1, 3, 1, 1]\n",
    "row_indices = [1, 3, 2, 0, 3, 1]\n",
    "column_indices = [0, 2, 3, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack this a bit.  The fact that `column_indices` has a 5 in element 3 (remember, we are assuming zero indexing), means that the index-3 column (really the forth column) starts at index 5 in the `row_indices` and `values` arrays; the fact that `column_indices` has a 2 in element 1 means that the index-1 column (really the second column) starts at index 2 in the `row_indices` and `values` arrays.\n",
    "\n",
    "The advantage to this format is that it is extremely efficient to look up _all_ the entries in a given column $i$.  If we want to know the entries of column `i`, we would use simply get the slice (using Python notation here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-e769f760ac6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcolumn_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "values[column_indices[i]:column_indices[i+1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the same could be done to get the row indices).  This also hopefully clarifies why the `column_indices` array contains $n+1$ entries: so that we can always use the range `column_indices[i]:column_indices[i+1]` to get all the items in the column (the last entry in `column_indices` must therefore be equal to the number of non-zero elements in the matrix.\n",
    "\n",
    "As is hopefully apparent, CSC format is typically much better than COO for quickly accessing elements, especially accessing single rows in the matrix (there is a corresponding compressed sparse row (CSR) that does the exact same thing but row-wise, for quick access to rows).  But conversely, it is very poor at adding new elements to the array (this requires shifting all subsequent items in the `row_indices` and `values` columns, and incrementing subsequent elements in the `column_indices` array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 3 0]\n",
      " [2 0 0 1]\n",
      " [0 1 0 0]\n",
      " [4 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "values = [2, 4, 1, 3, 1, 1]\n",
    "row_indices = [1, 3, 2, 0, 3, 1]\n",
    "column_indices = [0, 0, 1, 2, 2, 3]\n",
    "A = sp.coo_matrix((values, (row_indices, column_indices)), shape=(4,4))\n",
    "print(A.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly access the values, rows indices, and column indices of a COO sparse matrix via the `.data`, `.row`, and `.col` properties respectively (indeed, this is exactly how the sparse matrix is represented internally, with these three attributes).  Each of these are a 1D numpy array that store the data for the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 3 1 1]\n",
      "[1 3 2 0 3 1]\n",
      "[0 0 1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(A.data)\n",
    "print(A.row)\n",
    "print(A.col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily convert to CSC format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A.tocsc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a CSC matrix, the values are in still in the `.data` pointer, but the row indices and column indices (as we used the terms), are in the `.indices` and `.indptr` arrays; again, this are all just numpy arrays, that internally store the actual data of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 3 1 1]\n",
      "[1 3 2 0 3 1]\n",
      "[0 2 3 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(B.data)\n",
    "print(B.indices)\n",
    "print(B.indptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, let's create a 1000x1000 sparse matrix with 99.9% sparsity plus an identity matrix (we'll do this with the `sp.rand` call, which randomly chooses entries to fill in, and then makes samples them from a uniform distribution ... we add the identity to make the matrix likely to be invertible).  The precise nature of the matrix isn't important here, we just want to consider the timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 µs ± 521 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "388 µs ± 24.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "A = sp.rand(1000,1000, 0.001) + sp.eye(1000)\n",
    "B = np.asarray(A.todense())\n",
    "x = np.random.randn(1000)\n",
    "%timeit A @ x\n",
    "%timeit B @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the sparse version is about 50x faster, though of course the speedup will increase with sparsity relative to the dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.49 ms ± 62.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "34.9 ms ± 5.47 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse.linalg as spla\n",
    "A = A.tocsc()\n",
    "%timeit spla.spsolve(A,x)     # only works with CSC or CSR format\n",
    "%timeit np.linalg.solve(B,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
